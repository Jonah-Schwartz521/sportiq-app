name: Nightly NBA Scores Refresh

on:
  # Run every night at 10:30 AM UTC (3:30 AM America/Denver)
  schedule:
    - cron: '30 10 * * *'
  # Allow manual triggering for testing
  workflow_dispatch:

jobs:
  refresh-scores:
    runs-on: ubuntu-latest

    # Grant write permissions for creating PRs
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Cache NBA backfill state
        uses: actions/cache@v4
        with:
          path: model/data/processed/nba/nba_backfill_state.json
          key: nba-backfill-state-${{ hashFiles('model/scripts/backfill_nba_scores_2025_bdl.py') }}
          restore-keys: |
            nba-backfill-state-

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt
          python -m pip install --upgrade requests
          python -m pip install --upgrade pandas pyarrow
          python - <<'PY'
          import sys
          import pandas as pd
          import pyarrow  # noqa: F401
          import requests
          print('Python:', sys.version)
          print('sys.executable:', sys.executable)
          print('pandas:', pd.__version__)
          print('requests:', requests.__version__)
          PY

      - name: Capture baseline parquet stats (if exists)
        id: baseline
        run: |
          python - <<'PY'
          import pandas as pd
          from pathlib import Path
          path = Path('model/data/processed/nba/nba_games_with_scores.parquet')
          out = open('$GITHUB_OUTPUT', 'a')
          if not path.exists():
              print('baseline_seasons=0', file=out)
              print('baseline_rows=0', file=out)
              raise SystemExit
          df = pd.read_parquet(path)
          seasons = df['season'].nunique(dropna=True) if 'season' in df.columns else 0
          rows = len(df)
          print(f'baseline_seasons={seasons}', file=out)
          print(f'baseline_rows={rows}', file=out)
          print('Baseline rows:', rows)
          print('Baseline seasons:', seasons)
          PY

      - name: Warn if API key is missing
        run: |
          if [ -z "${{ secrets.BALLDONTLIE_API_KEY }}" ]; then
            echo "⚠️  BALLDONTLIE_API_KEY is not set; proceeding unauthenticated."
          else
            echo "✓ BALLDONTLIE_API_KEY configured."
          fi

      - name: Run NBA 2025 backfill
        env:
          BALLDONTLIE_API_KEY: ${{ secrets.BALLDONTLIE_API_KEY }}
        run: |
          python model/scripts/backfill_nba_scores_2025_bdl.py

      - name: Verify parquet
        run: |
          python - <<'PY'
          import pandas as pd
          from pathlib import Path
          import datetime as dt

          path = Path('model/data/processed/nba/nba_games_with_scores.parquet')
          if not path.exists():
              raise SystemExit('Parquet missing after backfill.')

          df = pd.read_parquet(path)
          if 'date' not in df.columns:
              raise SystemExit(f"Expected 'date' column in parquet, found: {list(df.columns)}")

          df['date'] = pd.to_datetime(df['date'], errors='coerce')
          final = (df['status'] == 'FINAL').sum() if 'status' in df.columns else 0

          now = dt.datetime.utcnow().replace(tzinfo=None)
          last7 = df[df['date'] >= (now - dt.timedelta(days=7))]
          final_last7 = (last7['status'] == 'FINAL').sum() if 'status' in last7.columns else 0

          scored = df[df.get('home_pts').notna() & df.get('away_pts').notna()] if 'home_pts' in df.columns and 'away_pts' in df.columns else df.iloc[0:0]

          seasons = df['season'].nunique(dropna=True) if 'season' in df.columns else 0
          rows = len(df)

          baseline_seasons = int("${{ steps.baseline.outputs.baseline_seasons || '0' }}")
          baseline_rows = int("${{ steps.baseline.outputs.baseline_rows || '0' }}")

          if baseline_seasons and seasons < baseline_seasons:
              raise SystemExit(f'Seasons dropped: before={baseline_seasons} after={seasons}')
          if baseline_rows and rows < baseline_rows * 0.95:
              raise SystemExit(f'Row count dropped too much: before={baseline_rows} after={rows}')

          print('Rows:', len(df))
          print('FINAL rows:', final)
          print('Max date:', df['date'].max())
          print('Max date with scores:', scored['date'].max() if not scored.empty else None)
          print('Rows last 7 days:', len(last7))
          print('FINAL last 7 days:', final_last7)
          PY

      - name: Check for changes
        id: git-diff
        run: |
          if git diff --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Stage intended files
        if: steps.git-diff.outputs.changed == 'true'
        id: stage-intended
        run: |
          git status --short
          git add model/data/processed/nba/nba_games_with_scores.parquet model/data/processed/nba/nba_backfill_state.json
          git diff --cached --stat || true
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Create Pull Request
        id: create-pr
        if: steps.stage-intended.outputs.changed == 'true'
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: 'chore: update NBA scores (automated backfill)'
          committer: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
          author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
          branch: bot/nba-scores-refresh
          delete-branch: false
          title: 'chore: update NBA scores (automated refresh)'
          body: |
            ## Automated NBA Scores Refresh

            This PR updates the NBA scores parquet file with the latest data.

            - **Workflow**: Nightly NBA Scores Refresh
            - **Trigger**: ${{ github.event_name }}
            - **File**: `model/data/processed/nba/nba_games_with_scores.parquet`

            This is an automated nightly refresh. Review and merge if the data looks correct.
          labels: |
            automated
            data-refresh

      - name: No changes detected
        if: steps.git-diff.outputs.changed != 'true'
        run: |
          echo "No changes detected; no PR created."


# File: model/scripts/backfill_nba_scores_2025_bdl.py

import pandas as pd
import requests
import os
from pathlib import Path
import time
import sys

def _ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure df has a `date` datetime64 column. Accept common alternate column names."""
    if df is None:
        return pd.DataFrame({"date": pd.to_datetime(pd.Series([], dtype="datetime64[ns]"), errors="coerce")})

    if df.empty:
        if "date" not in df.columns:
            df = df.copy()
            df["date"] = pd.to_datetime(pd.Series([], dtype="datetime64[ns]"), errors="coerce")
        else:
            df = df.copy()
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
        return df

    df = df.copy()
    if "date" not in df.columns:
        for alt in [
            "game_date",
            "start_date",
            "start_time",
            "startTime",
            "datetime",
            "scheduled",
            "commence_time",
            "tipoff_time",
            "time",
            "utc_start",
            "start_at",
        ]:
            if alt in df.columns:
                df = df.rename(columns={alt: "date"})
                break

    if "date" not in df.columns:
        df["date"] = pd.NaT

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    return df

def main():
    output_path = Path("model/data/processed/nba/nba_games_with_scores.parquet")
    existing = None
    if output_path.exists():
        existing = pd.read_parquet(output_path)
    else:
        existing = pd.DataFrame()

    existing = _ensure_date_column(existing)

    # BallDontLie API details
    API_URL = "https://www.balldontlie.io/api/v1/games"
    API_KEY = os.environ.get("BALLDONTLIE_API_KEY", None)

    # We want to backfill the 2025 season games (assuming season=2025)
    target_season = 2025

    # Pagination params
    per_page = 100
    page = 1

    all_games = []

    headers = {}
    if API_KEY:
        headers["Authorization"] = f"Bearer {API_KEY}"

    # Fetch all games for the target season via pagination
    while True:
        params = {
            "seasons[]": target_season,
            "per_page": per_page,
            "page": page,
        }
        try:
            response = requests.get(API_URL, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            print(f"Error fetching page {page}: {e}", file=sys.stderr)
            # Backoff and retry logic
            time.sleep(10)
            continue

        games = data.get("data", [])
        if not games:
            break

        all_games.extend(games)

        meta = data.get("meta", {})
        total_pages = meta.get("total_pages", page)
        if page >= total_pages:
            break
        page += 1
        time.sleep(0.1)  # be gentle on the API

    if not all_games:
        print("No games fetched for season 2025.")
        return

    # Normalize JSON data into DataFrame
    incoming = pd.json_normalize(all_games)

    # Ensure date column exists and is datetime
    incoming = _ensure_date_column(incoming)

    # Remove duplicates that already exist based on game 'id'
    if not existing.empty and "id" in existing.columns:
        existing_ids = set(existing["id"].dropna().unique())
        incoming = incoming[~incoming["id"].isin(existing_ids)]

    # Combine existing and incoming data
    merged = pd.concat([existing, incoming], ignore_index=True)

    merged = _ensure_date_column(merged)

    # Sort by date and id for consistency
    if "date" in merged.columns and "id" in merged.columns:
        merged = merged.sort_values(by=["date", "id"]).reset_index(drop=True)
    elif "date" in merged.columns:
        merged = merged.sort_values(by="date").reset_index(drop=True)
    elif "id" in merged.columns:
        merged = merged.sort_values(by="id").reset_index(drop=True)

    # Write to parquet
    merged.to_parquet(output_path, index=False)

if __name__ == "__main__":
    main()
